---
title: "K-medoids"
output: github_document
---

An alternative to k-means is the k-medoids or partitioning around medoids (PAM) algorithm.

According to [wikipeadia](https://en.wikipedia.org/wiki/K-medoids): 

> Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses data points as centers (medoids or exemplars) and can be used with arbitrary distances, while in k-means the centre of a cluster is not necessarily one of the input data points (it is the average between the points in the cluster).

K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster.

The k-methods method is more robust to noise and outliers. The medoid used is comparable to a median rather than a mean. Median is considered more robust to outliers than using the mean.

Disadvantage: PAM can take much longer to run than k-means. As it involves computing all pairwise distances, it is O(n^2*k*i); whereas k-means runs in O(n*k*i) 


```{r}
library(tidyverse)
library(cluster)

```



```{r}
data(iris)

str(iris)
```

```{r}
fitted <- pam(iris, k = 3, metric = "manhattan")

fitted

```



